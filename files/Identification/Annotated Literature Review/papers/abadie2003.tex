%!TEX root = /Users/manunavjeevan/Documents/GitHub/mnavjeev.github.io/files/Identification/Annotated Literature Review/identificationLitReview.tex

\newpage
\section{Semiparametric Instrumental Variable Estimation of Treatment Response Models \textit{\small Alberto Abadie (JoE, 2003)}}

\subsection{Introduction}

Economists long concrened with how to estimate the effect of a treatment on some outcome of interest, possilbe after conditioning on a vector of covariates. Main empirical challenge in studies of this type arises from the fact that selection for treatment is usually related to the potential outcomes that individuals would attain with and without the treatment. 

Variety of methods have been proposed to overcome the selection problem. Traditional approach relies on distributional assumptions and functional form restrictions to identify average treatment effects and other treatment parameters of interest. Estimators based on this approach can be seriously biased by modest departures from parametric assumptions. In addition, a number of researchers have noted that strong parametric assumptions are not necessary to identify parameters of interest. 

Consequently, desirable to develop robust estimators of treatment parameters bases on nonparametric or semi parametric identification procedures. Article introduces a new class of IV estimators of linear and nonlinear average treatment response models with covariates. In the spirit of Roehrig (1988), identification is attained non-parametrically and does not depend on the choice of parametric model. As in the IV model of IA (1994), identification comes from a binary instrument that induces exogenous selection into treatment for some subset of the population. Approach taken here easily accommodates covariates and can be used to estimate nonlinear models with a binary and endogenous regressor. 

Ability to control for covariates is important because instruments may require conditioning on a set of covariates to be valid. Covariates can also be used to reflect observable differences in the composition of populations. As a by-product of the general framework introduced here, develop an IV estimator that provides a linear least squares approximation to an average treatment response function, just as OLS provides a linear least squares approximation to a conditional expectation. Shown that 2SLS typically does not have this property. 

\subsection{The Framework}
\subsubsection{Identification Problem}

Suppose interested in the effect of some treatment, represented by the binary variable $D$ (college) on some outcome of interest $Y$ (earnings). As in Rubin (1974, 1977) define $Y_1, Y_0$ as the potential outcomes that an individual would attain with and without being exposed to the treatment. Treatment parameters are defiend as characteristics of the distribution of $(Y_1, Y_0)$ for well defined subpopulations. 

In the example, $Y_1$ represents potential earnings as a college graduate while $Y_0$ represents potential earnings as a non-graduate. Treatment effect is $Y_1 - Y_0$. Now, an identification problem arises from the fact that we cannot observe both potential outcomes $Y_1$ and $Y_0$ for the same individual, only observe $Y = Y_1 D + Y_0 (1-D)$. Since one of the potential outcomes is always missing, cannot compute the treatment effect for any individual. 

However, comparisons of average earnings of average effect of the treatment on the treated and non treated to not usually give the right answer
\begin{equation}
	\label{eq:abadie-1}
	\begin{split}
		\E[Y|D=1] - \E[Y | D = 0] &= \E[Y_1 | D= 1] - \E[Y_0 | D=0] \\
								  &= \E[Y_1 - Y_0| D= 1] + \left(\E[Y_0|D=1] -\E[Y_0|D=0]\right)
	\end{split}
\end{equation}
Because treatment status is not independent of potential outcomes, this cannot be reduced to ATE. The first term of the RHS of \eqref{eq:abadie-1} gives the average effect of the treatment on the treated. Second term represents bias caused by selection into treatment. 

Of course, can think of other parameters of interest. 

\subsubsection{Identification by Instrumental Variables}

IV methods proposed to recover treatment parameters. Article follows the approach of Imbens and Angrist (1994).

Suppose that there is a binary instrument $Z$ available to the researcher. The formal requisites to be an instrument are stated below. Informally, the role of an instrument is to induce and exogenous variation in the treatment variable. The binary variable $D_z$ represents potential treatment status given $Z = z$. Suppose, for example, that $Z$ is an indicator for college proximity. Then $D_0 = 0$ and $D_1 = 1$, living nearby a college at the end of high school, but would not graduate otherwise. The treatment status indicator variable can then be expressed as $D = ZD_1 + (1-Z)D_0$. In practice, observe $Z$ and $D$, but not the potential treatment indicators. Following terminology of Angrist (1996), population is divided into groups by treatment indicators, compliers, never takers, always takers, defiers. 

In order to state the properties that a valid instrument should have, need to include $Z$ in the definition of the potential outcomes. For a particular individual, the variable $Y_{zd}$ represents the potential outcome would obtain if $D_0 = 0$ for some individual.

\begin{assumption}
	\label{assm:abadie-2.1}
	Assume the following on the joint distribution of observables and unobservables
	\begin{enumerate}
		\item Independence of the instrument: Conditional on $X$, the random vector $(Y_{00}, Y_{01}, Y_{10}, D_0, D_1)$ is independent of the instrument $Z$.
		\item Exclusion of the instrument: $\P(Y_{1d} = Y_{0d} | X) = 1$ for $d\in\{0,1\}$
		\item First Stage: $0 < \P(Z=1|X) < 1$ and $\P(D_1 = 1 | X) > \P(D_0 = 1 | X)$
		\item Monotonicity: $\P(D_1 \geq D_0 | X) = 1$
	\end{enumerate}
\end{assumption}
Assumptions are essentially the conditional versions of those used in Angrist. Assumption 2.1.2 means that we can write potential outcomes in terms of $D$.

Imbens and Angrist (1994) show that if assumption 2.1 holds, in absence of covariates, a simple IV estimand identifies the LATE 
\begin{equation}
	\label{eq:abadie-2}
	\alpha_{IV} = \frac{\cov(Y,Z)}{\cov(D,Z)} = \frac{\E[Y|Z=1]-\E[Y|Z=0]}{\E[D|Z=1]-\E[D|Z=0]} = \E[Y_1 - Y_0|D_1 > D_0]
\end{equation}

\subsection{Identification of Statistical Characteristics for Compliers}

Section presents an identification theorem that includes previous results on IV models for treatment effects on IV models for treatment effects as special cases. To study identification, proceed as if we know the joint distribution of $(Y,D,X,Z)$. In practice, can use a random sample from $(Y,D,X,Z)$ to construct empirical analogs.

\begin{lemma}
	\label{lemma:abadie-2.1}
	Under Assumption~\ref{assm:abadie-2.1}
	\[\P(D_1 > D_0|X) = \E[D|Z=1,X] - \E[D|Z=0,X] > 0\]
\end{lemma}
\begin{proof}
	Under Assumption~\ref{assm:abadie-2.1} 
	\begin{align*}
		\P(D_1 > D_0 | X) &= 1 - \P(D_1 = D_0 = 1| X) - \P(D_1 = D_0 = 1| X) &\parbox[t]{20em}{(by monotonicity)} \\
						  &= 1 - \P(D_1 = D_0 = 0 | X, Z = 1) - \P(D_1 = D_0 = 1 | X, Z=0) &\parbox[t]{20em}{(by independence of $Z$)} \\ 
						  &= 1 - \P(D=0|Z,Z= 1) - \P(D=1|X,Z=0) &\parbox[t]{20em}{(by monotonicity)} \\
						  &= \P(D= 1|X,Z=1) - \P(D=1|X,Z=0) &\parbox[t]{20em}{(because $D$ is binary)} \\
						  &= \E[D | X, Z= 1] - \E[D|X,Z=0] &\parbox[t]{20em}{(because $D$ is binary)}
	\end{align*}
\end{proof}
Lemma says that, under assumption 2.1, proportion of compliers in the population is identified given $X$ and that this proportion is greater than 0. Preliminary result is important for establishing the following theorem. 

\begin{theorem}
	\label{thm:abadie-3.1}
	Let $g(\cdot)$ be any measurable function of $(Y,D,X)$ such that $\E[|g(Y,D,X)|] < \infty$. Define 
	\begin{align*}
		\kappa_{(0)} &\equiv (1-D)\frac{(1-Z) - \E[(1-Z)|X]}{\E[(1-Z) | X]\E[Z|X]} \\ 
		\kappa_{(1)} &\equiv D\frac{Z - \E[Z|X]}{\E[(1-Z) | X]\E[Z|X]} \\ 
			  \kappa &\equiv \kappa_{(0)}\E[(1-Z)|X] + \kappa_{(1)}\E[Z|X] \\
			  		 &= 1 - \frac{D(1-Z)}{\P(Z= 0|X)} - \frac{(1-D)Z}{\P(Z=1|X)}
	\end{align*}
	Then under Assumption~\ref{assm:abadie-2.1}
	\begin{align}
		\E[g(Y,D,X)|D_1 > D_0] &= \frac{1}{\P(D_1 > D_0)}\E\left[\kappa g(Y,D,X)\right]\\
		\E[g(Y_0,X)|D_1 > D_0] &= \frac{1}{\P(D_1 > D_0)}\E\left[\kappa_{(0)}g(Y,X)\right] \\
		\E[g(Y_1,X)|D_1 > D_0] &= \frac{1}{\P(D_1 > D_0)}\E\left[\kappa_{(1)}g(Y,X)\right]
	\end{align}
\end{theorem}
This theorem is a powerful identification result, says that any statistical characteristic that can be defined in terms of moments of the joint distribution is identified for compliers. \emph{Since $D$ is exogenous given $X$ for compliers, Theorem~\ref{thm:abadie-3.1} can be used to identify meaningful treatment parameters for used to identify meaningful treatment parameters for this group of the population.}

Now go through the proof of this Theorem. 
\begin{proof}
	Monotonicity along with the law of total expectation implies that 
	\begin{equation*}
		\begin{split}
			\E[g(Y,D,X) | X, D_1 > D_0] &= \frac{1}{\P(D_1 > D_0|X)}\Big\{\E[g(Y,D,X)|X] \\
										&- \E[g(Y,D,X)|X,D_1= D_0 = 1]\P(D_1 = D_0 = 1|X) \\
										&- \E[g(Y,D,X)|X,D_1= D_0 = 0]\P(D_1 = D_0 = 0|X)\Big\}
		\end{split}
	\end{equation*}
	Since $Z$ is ignorable and independent of the potential outcomes given $X$ and since monotonicity is assumed, the above equation can be rewritten as
	\begin{equation*}
		\begin{split}
			\E[g(Y,D,X) | X, D_1 > D_0] &= \frac{1}{\P(D_1 > D_0|X)}\Big\{\E[g(Y,D,X)|X] \\
										&- \E[g(Y,D,X)|X,D=1, Z= 0]\P(D = 1|X, Z= 0) \\
										&- \E[g(Y,D,X)|X,D = 0, Z= 1]\P(D = 0|X, Z = 1)\Big\}
		\end{split}
	\end{equation*}
	Consider similarly, by law of total expectation
	\begin{equation*}
		\begin{split}
			\E[D(1-Z)g(Y,D,X) | X] &= \E[g(Y,D,X)|X,D=1, Z=0]\P(D=0,Z=1|X) \\ 
								   &= \E[g(Y,D,X)|X,D=1, Z=0]\P(D=1|X,Z=0)\P(Z=0|X)\\
			\implies \frac{1}{\P(Z=0|X)}\E[D(1-Z)g(Y,D,X) | X] &= \E[g(Y,D,X)|X,D=1, Z=0]\P(D=1|X,Z=0)\\
			\E[Z(1-D)g(y,D,X)| X]  &= \E[g(Y,D,X)|X,D=0, Z=1]\P(D=0,Z=1)\\
								   &= \E[g(Y,D,X)|X,D=0, Z=1]\P(D=0|X,Z=1)\P(Z=1|X)\\
			\implies \frac{1}{\P(Z=1|X)}\E[Z(1-D)g(y,D,X)| X] &=  \E[g(Y,D,X)|X,D=0, Z=1]\P(D=0|X,Z=1)
		\end{split}
	\end{equation*}
	Under Assumption~\ref{assm:abadie-2.1}.3 can combine the last three equations to 
	\begin{equation*}
		\E[g(Y,D,X) | X, D_1 > D_0] = \frac{1}{\P(D_1 > D_0|X)}\E\left[g(Y,D,X)\left(1 - \frac{D(1-Z)}{\P(Z=0|X)} - \frac{Z(1-D)}{\P(Z=1|X)}\right)\bigg| X\right] \\ 
	\end{equation*}
	Bayes' Theorem implies that 
	\begin{equation*}
		\begin{split}
			\P(D_1 > D_0 | X) &= \frac{f_{X|D_1>D_0}(x)\P(D_1 > D_0)}{f_X(x)} \\
			\implies \frac{1}{\P(D_1 > D_0 | X)} &= \frac{f_X(x)}{f_{X|D_1>D_0}(x)\P(D_1 > D_0)}
		\end{split}
	\end{equation*}
	Applying Bayes' Theorem and Integrating yields
	\begin{equation*}
		\begin{split}
			&\int \E[g(Y,D,X) | X, D_1 > D_0] dP(X|D_1 > D_0) \\
			&= \int  \E[g(Y,D,X) | X, D_1 > D_0] f_{X | D_1 > D_0}(x)dx\\
			&= \frac{1}{\P(D_1 > D_0)} \int \E\left[g(Y,D,X)\left(1 - \frac{D(1-Z)}{\P(Z=0|X)} - \frac{Z(1-D)}{\P(Z=1|X)}\right)\bigg| X\right] dP(X)
		\end{split}
	\end{equation*}
	or 
	\begin{equation*}
		\E[g(Y,D,X)|D_1 > D_0] = \frac{1}{\P(D_1 > D_0)}\E[\kappa g(Y,D,X)]
	\end{equation*}
	This proves the first part of the theorem. To prove the second part, note that 
	\begin{align*}
		\E[g(Y,X)(1-D)|X, D_1 > D_0] &= \E[g(Y_0, X)|D=0, X, D_1 > D_0]\P(D=0|X, D_1 > D_0) & \\ 
									 &= \E[g(Y_0, X)|Z=0, X, D_1 > D_0]\P(D=0|X,D_1 > D_0)  &\parbox[t]{20em}{(for compliers $Z = D$)} \\
									 &= \E[g(Y_0, X)|X,D_1 > D_0]\P(Z=0|X) &\parbox[t]{20em}{(by independence of $Z$)}
	\end{align*}
	The proof of the next parts follows quickly from here. For the second equality note that 
	\begin{align*}
		\E[g(Y_0, X) | X, D_1 > D_0] &= \E\left[g(Y,X)\frac{(1-D)}{\P(Z= 0|X)}\Big| X, D_1 > D_0\right] \\ 
									 &= \frac{1}{\P(D_1 > D_0 |X)}\E[\kappa\frac{(1-D)}{\P(Z=0|X)}g(Y,X) | X] \\ 
									 &= \frac{1}{\P(D_1 > D_0 |X)}\E[\kappa_0 g(Y,X)|X]
	\end{align*}
	Integration of this equation as before yields the result.
\end{proof}

\subsection{Estimation of Average Response Functions}

\subsubsection{Local Average Response Functions}

Consider the function of $(D,X)$ that is equal to $\E[Y_0 | X, D_1 > D_0]$ if $D = 0$ and is equal to $\E[Y_1 | X, D_1 > D_0]$ if $D = 1$. This function describes average treatment responses for any group of compliers defined by some value for the covariates. Refer to this function as the local average response function (LARF). Since $Z = D$ for compliers, under Assumptions~\ref{assm:abadie-2.1}.1 and \ref{assm:abadie-2.1}.2, $Z$ is ignorable for the compliers given $X$. It follows that
\begin{align*}
	\E[Y | X, D= 0, D_1 > D_0] &= \E[Y_0 | D_1 > D_0] \\
	\E[Y | X, D=1, D_1 > D_0] &= \E[Y_1 | X, D_1 > D_0] \\
	\E[Y | X, D=1, D_1 > D_0] - \E[Y|X, D= 0, D_1 > D_0] &= \E[Y_1 - Y_0|X, D_1 > D_0]
\end{align*}
So $\E[Y | X, D_1 > D_0]$ is the LARF. Important special case arises when $\P(D_0 = 0 | X) = 1$. This happens in some randomized experiments.

The face that the conditional expectation of $Y$ given $D$ and $X$ holds for compliers has an interpretation as an average treatment response function would not be very useful in the absence of Theorem~\ref{thm:abadie-3.1}. 

\subsubsection{Estimation}

Section describes two ways to learn about the LARF: (i) estimate a parameterization of the LARF by Least Squares, (ii) Specify a parametric distribution for $P(Y|X,D,D_1>D_0)$ and estimate the parameters of the LARF by a Maximum Likelihood. Since identification does not depend on these parametric specifications have an appealing interpretations under misspecification of the parameterization in (i) or (ii). Through this $W = (Y,D,X,Z)$ and $\{w_i\}_{i=1}^n$ is a sample of realizations of $W$. 

\paragraph{Least Squares}

Suppose that the LARF belongs to some class of parametric functions $\mathcal{H} = \{h(D,X;\theta): \theta \in \Theta \subset \SR^m\}$ in the Lebesgue space of square-integrable functions. Let $\theta_0$ be the vector of parameters such that $\E[Y|X,D,D_1> D_0] = h(D,X;\theta)$. Then
\begin{equation}
	\label{eq:abdaie-5}
	\begin{split}
		\theta_0 &= \arg\min_{\theta \in \Theta} \E\left[(Y-h(D,X;\theta))^2|D_1 > D_0\right] \\
		&= \arg\min_{\theta\in\Theta}\E\left[\kappa(Y-h(D,X;\theta))^2\right]
	\end{split}
\end{equation}
Under functional form misspecification (LARF does not belong to $\calH$), $\theta_0$ are the parameters of the best least squares approximation from $\calH$ to $\E[Y|D,X,D_1 > D_0]$:
\begin{equation*}
	\theta_0 = \arg\min_{\theta\in\Theta} \E\left[\left(\E[Y|D,X,D_1 > D_0] - h(D,X;\theta)\right)^2\right]
\end{equation*}









