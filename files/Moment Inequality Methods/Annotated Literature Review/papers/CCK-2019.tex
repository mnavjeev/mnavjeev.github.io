
%!TEX root = /Users/manunavjeevan/Desktop/Research/Moment Inequality Methods/Annotated Literature Review/inequalityLitReview.tex

\newpage
\section{Inference on Causal and Structural Parameters Using Many Moment Inequalities; \textit{\small Victor Chernozhukov, Denis Chetverikov, and Kengo Kato (ReStud, 2018)}}\label{sec:CCK-2019}

\citet{CCK-2018} covers inference on many moment inequalities. Builds on prior work on high dimensional CLT.

\subsection{Introduction}

In recent years, moment inequalities framework has developed into a powerful tool for inference on causal and structural parameters in partially identified models. Many papers study models with a finite and fixed number of conditional and unconditional moment inequalities. IN practice the number of moment inequalities implied by the model is often large. 

Examples of testing (very) many moment inequalities
\begin{itemize}
	\item Consumer is selecting a bundle of products for purchase and moment inequalities come from revealed preference argument (Pakes, 2010)
	\item Market structure model of Ciliberto and Tamer (2009), number of moment inequalities equals the number of possible combinations of firms that could potentially enter the market (grows exponentially in the number of firms)
	\item Dynamic model of imperfect competition of Bajari, Benkard, Levin (2007)m where deviations from optimal policy serve to define many moment inequalities
	\item Beresteanu, Molchanov, Molinari (2011), Galichon and Henry (2011)\footnote{This seems like a good place to start reading}, Chesher, Rosen, Smolinksi (2013), and Chester and Rosen (2013)
\end{itemize}
Many examples have inportant in that the many inequalities under consideration are ``unstructured'', they cannot be viewed as unconditional moment inequalities generated froma small number of conditional inequalities with a low-dimensional conditioning variable. So existing inference methods for conditional moment inequalities, though fruitful in many cases 

Formally describing the problem, let $\{X_i\}_{i=1}^n$ be a sequence of i.i.d random vectors in $\mathbb{R}^p$, where $X_i = (X_{i1},\dots,X_{ip})^T$, with a common distribution denoted by $\mathscr{L}_X$. For $j \leq p$, we write $\mu_j := \mathbb{E}[X_{1j}]$. Interested in testing the null hypothesis 
\begin{equation}
	\label{eq:CCK-1}
	H_0 :\mu_j \leq 0\hbox{ }\text{ for all }j = 1,\dots,p
\end{equation}
Against the alternative 
\begin{equation}
	\label{eq:CCK-2}
	H_1: \mu_j > 0 \hbox{ }\text{ for some }j = 1,\dots,p
\end{equation}
Refer to (\ref{eq:CCK-1}) as the moment inequalities and say the $j$th moment is satisfied (viloated) if $\mu_j \leq 0$ ($\mu_j > 0$). Paper will allow number of moment inequalities $p \gg n$. Consider a test statistic given by the maximum over $p$ Studentized (t-type) inequality specific statistic. Consider critical values based upon (i) the union bound combined with a moderate deviation inequality for self-normalized sums and (ii) bootstrap methods. Among bootstrap methods, consider multiplier and emppirical bootstrap methods. These are simulation based and computationally more difficult, but take into account correlation structure and yeild lower critical values. SN method is particularly useful for grid search when the researcher is interested in constricting a confidence interval for identified set. 

Also consider two-step methods incorporating inequality selection procedutes. Two-step methods get rid of most uninformative inequalities, that is inequalities with $\mu_j < 0$ if $\mu_j$ is not too close to 0. Also develop novel three-step methods by incorporating double inequality selection procedures. These are suitable in parametric modesl defined via momoent inequalities and allow to drop weakly informative inequalities in addition to uninformative inequalities.\footnote{Can be extended to nonparametric models as well}. Results can be used for construction of confidence regions for identifiable parameters in partially identified models defined by moment inequalities. Show that results are asymptotically honest (don't quite know what this means).

Literature testing unconditional moment inequalities is large. See White (2000), Chernozhukov, Hong, and Tamer (2007), Romano and Shaikh (2008), Rosen (2008), Andrews and Guggenberger (2009), Andrews and Soares (2010), Canay (2010), Bugni (2011), Andrews and Jia-Barwick (2012), and Romano, Shaikh, and Wolf (2014). 

In this paper we implicitly assume that $X_1, \dots, X_n$ and $p$ are indexed by $n$. Mainly interested in the case that $p = p_n \rightarrow \infty$ as $n \rightarrow \infty$

\subsection{Motivating Examples}

Section provides examples that motivate the framework where the number of moment inequalities $p$ is large and potentially much larger than the sample size $n$. In these examples, one actually has many conditional rather than unconditional inequalities. Results cover conditioning as well. 

\subsubsection{Market Structure Model}

Let $m$ denote the number of firms that could potentially enter the market. Let $m$-tuple $D = (D_1, \dots, D_m)$ denote entry decisions of these firms. That is, $D_j = 1$ if the firm $j$ enters the market and $D_j = 0$ otherwise. Let $\mathscr{D}$ denote the possible values of $D$. We have that $|\mathscr{D}| = 2^m$. 

Let $X$ and $\epsilon$ denote the (exogeneous) characteristics of the market as well as characteristics of the firms that are observed and not observed by the researcher, respectively. The profit of the firm $j$ is given by 
$$\pi_j(D, X, \epsilon, \theta)$$
where $\pi_j$ is known up to a parameter $\theta$. Both $X$ and $\epsilon$ are observed by the firms and a Nash Equilibrium is played so that, for each $j$,
$$\pi_j((D_j, D_{-j}),X, \epsilon, \theta) \geq \pi_j((1-D_j, D_{-j}), X, \epsilon, \theta)$$
$D_{-j}$ denotes the decisions of all firms excluding the firm $j$. Then one can find set-valued functions $R_1(d, X, \theta)$ and $R_2(d, X, \theta)$ such that $d$ is the unique equilibrium whenever $\epsilon \in D_1(d,X, \theta)$ and $d$ is \textit{an} equilibrium whenever $\epsilon \in R_2(d,X, \theta)$. In the second case, the probability that the researcher sees $d$ as an equilibrium depends on the equilibrium selection mechanism. Without further information, anything can be in $[0,1]$. Therefore we have the following bounds 
\begin{align*}
	\mathbb{E}[\mathds{1}\{\epsilon \in R_1(d,X, \theta\}|X] &\leq \mathbb{E}[{\mathds{1}\{D=d\}|X}] \\
	&\leq \mathbb{E}[\mathds{1}\{\epsilon \in R_1(d,X,\theta) \cup R_2(d,X,\theta) \} | X]
\end{align*}
Further assuming that the conditional distribution of $\epsilon$ given $X$ is known (or known up to a parameter that is part of $\theta$), both the LHS and RHS of these inequalites can be calculated. Denote them $P_1(d, X, \theta)$ and $P_2(d,X,\theta)$, respectively to obtain 
\begin{equation}
	\label{2:CilbertoTamerInequalities}
	P_1(d, X, \theta) \leq \mathbb{E}[\mathds{1}\{D=d\}|X] \leq P_2(d,X,\theta)
\end{equation}
for all $d \in \mathds{D}$. These can be used for inference on the parameter $\theta$. Note that the number of inequalities in (\ref{2:CilbertoTamerInequalities}) is $2|\mathscr{D}| = 2^{m+1}$. This is a large number, even if $m$ is moderately large. Moreover, these inequalities are conditional on $X$. So, they can be transformed into a large and increasing number of unconditional moment inequalities as described above. Also, if the firms have more than two decisions, the number of inequalities will be even larger. 

Some other examples are given, but I won't cover them in notes. 

\subsection{Test Statistic}

Begin preparing some notation. Assume that 
\begin{equation}
	\label{eq:CCK-12}
	\mathbb{E}[X_{1,j}^2] < \infty, \sigma_j^2 := \Var(X_{1,j}) > 0, j = 1,\dots, p
\end{equation}
For $j = 1,\dots,p$ let $\muH_j$ and $\sigmaH_j$ be the sample mean and variance of $\{X_{i,j}\}_{i=1}^n$. Many different possible test statistics. Somewhat natural to consider statistics that take large values thehn some of $\hat{\mu}_j$'s are large. In this paper focus on statistic that takes large values when at least one of $\muH_j$ are large. 

In specific, focus on the following test statistic:
\begin{equation}
	\label{2:testStat}
	T = \max_{1\leq j \leq p}\frac{\sqrt{n}\muH_j}{\sigmaH_j}
\end{equation}
Large values of $T$ indicate a likely violation of $H_0$, so it is natural to consider tests of the form 
\[T > c \implies \text{ reject } H_0\]
where $c$ is appropriately chosen so that the test approximately has size $\alpha \in (0,1)$. Consider various ways for calculating critical values and prove their validity. 

\subsection{Critical Values}

Now move to define critical values for $T$ such that under $H_0$, the probability of rejecting $H_0$ does not exceed size $\alpha$ asymptotically. Methods are ordered bu increasing computational complexity, increasing strength of required conditions, and also increasing power. Basic idea for the construction of critical values for $T$ lies in the fact, that, under $H_0$:
\[T \leq \max_{1\leq j \leq p} \frac{\sqrt{n}(\muH_j - \mu_j)}{\sigmaH_j} \]
Consider two approaches to constructing such critical values: self-normalized and bootstrap methods. Also consider two- and three-step variants of the methods by incorporating inequality selection. 

Following notation used:
\[Z_{ij} = (X_{ij} - \mu_j)/\sigma_j\hbox{ }\text{ and }\hbox{ }Z_i = (Z_{i1}, \dots, Z_{ip})^T \]
Observe that $\mathbb{E}[Z_{ij}] = 0$ and $\mathbb{E}[Z_{ij}^2] = 1$. Define 
\[M_{n,k} = \max_{1\leq j \leq p}\left( \mathbb{E}\left[|Z_{1,j}|^k\right]   \right)^{1/k},k = 3,4,\hbox{ }\text{ and }B_n = \left(\mathbb{E}\left[\max_{1\leq j \leq p} Z_{1j}^4     \right]   \right)^{1/4} \]
The dependence on $n$ comes via the dependence of $p = p_n$ on n implicitly. By Jensen's inequality, $B_n \geq M_{n,4} \geq M_{n,3} \geq 1$. In addition, if all $Z_{ij}$'s are bounded a.s by a constant $C$, we have that $C \geq B_n$. These are useful to get a sense of various conditions on $M_{n,3}, M_{n,4}$ and $B_n$ imposed in the theorems below. 

\subsubsection{Self Normalized Critical Values}
\paragraph{One-step method:}

Self-normalized method considered is based on the union bound combined with moderate deviation inequality for self-normalized sums. Under $H_0$
\[\mathbb{P}(T > c) \leq \sum_{j=1}^p \mathbb{P}(\sqrt{n}(\muH_j -\mu_j)/\sigmaH_j > c) \numberthis\]
This bound seems crude when $p$ is large. However, will exploit the self normalizing $\sqrt{n}(\hat{\mu}_j - \mu_j)/\sigmaH_j$ to show that RHS of above is bounded, even if $c$ is growing logarithmically fast with $p$. Using such a $c$ will yield a test with better power properties. 

For $j = 1,\dots, p$, define 
\[U_j := \sqrt{n}\mathbb{E}_n[Z_{ij}]/\sqrt{\mathbb{E}_n[Z_{ij}^2]}\]

Simple algebra yields, we see that 
\[\sqrt{n}(\muH_j - \mu_j)/\sigmaH_j = U_j/\sqrt{1 - U_j^2/n} \]

where the right-hand side is increasing in $U_j$ as long as $U_j \geq 0$. So under $H_0$, 
\[\mathbb{P}(T > c) \leq \sum_{j=1}^p \mathbb{P}\left( U_j > c/\sqrt{1 + c^2/n}   \right), \hbox{ }c \geq 0 \numberthis \]
Moderate deviation inequality for self-normalized sums of Jing, Shao, and Wang (2003) implies that for moderatley large $c\geq 0$,
\[\mathbb{P}\left(U_j > c/\sqrt{1 + c^2/n}\right) \approx \mathbb{P}\left(Z > x/\sqrt{1 + c^2/n}\right) \]
where $Z \sim N(0,1)$. The above approximation holds even if $Z_{ij}$ only have $2 + \delta$ finite moments for some $\delta > 0$. Therefore, take the critical value as 
\begin{equation}
	\label{eq:cSN}
	c^{SN}(\alpha) = \frac{\Phi^{-1}(1 - \alpha/p)}{\sqrt{1 - \Phi^{-1}(1- \alpha/p)^2/n}} 
\end{equation}
where $\Phi(\cdot)$ is the normal cdf. We call $c^{SN}(\alpha)$ the one-step SN critical value with size $\alpha$ as it's derivation depends on the moderate deviation inequality for self-normalized sums. Note that 
\[\Phi^{-1}(1-\alpha/p) \sim \sqrt{\log(p/\alpha)}\]
so $c^{SN}(\alpha)$ depends on $p$ only through $\log(p)$. Following theorem provides a non asymptotic bound on the probability that the rest statsistic $T$ exceeds the SN critical value $c^{SN}(\alpha)$ under $H_0$ and shows that the bound converged to $\alpha$ under mild regularity conditions, validating the SN method. 

\begin{theorem}
	(Validity of one-step SN method). Suppose that $M_{n,3}\Phi^{-1}(1-\alpha/p) \leq n^{1/6}$. Then under $H_0$, 
	\[\mathbb{P}(T > c^{SN}(\alpha)) \leq \alpha\left[1 + Kn^{-1/2}M_{n,3}^3\left\{1 + \Phi^{-1}(1-\alpha/p) \right\}^3\right]\]
	where $K$ is a universal constant. Hence, if there exists constants $0 < c_1 < 1/2$ and $C_1 > 0$ such that 
	\[M_{n,3}^3\log^{3/2}(p/\alpha) \leq C_1 n^{1/2 - c_1}\numberthis\]
	then there exists a positive constant $C$ depending only on $C_1$ such that under $H_0$,
	\[\mathbb{P}(T > c^{SN}(\alpha)) \leq \alpha + Cn_{-c_1}\numberthis\]
	Moreover, this bound holds uniformly over all distributions $\mathscr{L_X}$ satisfying the moment conditions as well as the above requirement (9). In addition, if (9) holds, all components of $X_1$ are independent, $\mu_j = 0$ for all $1 \leq j \leq p$ and $p = p_n \rightarrow \infty$, then 
	\[\mathbb{P}(T > c^{SN}(\alpha)) \rightarrow 1 - e^{-\alpha}\]
\end{theorem}
I think the last bit is just to show that the test is approximately non-conservative. 

\paragraph{Two-step method:} Now move to combine the SN method with inequality selection. Motivation for doing this is that when $\mu_j < 0$ for some $j = 1, \dots, p$ the inequality in (6) becomes strict. So, when there are many $j$ for which $\mu_j$ are negative and large in absolute value, the resulting rest with one-step SN critical values would tend to be unnecessarily conservative. So, in order to improve the power of the test, it is better to exclude $j$ for which $\mu_j$ are below some (negative) threshold when computing critical values.  

Formally, let $0 < \beta_n < \alpha/2$ be some constant. For generality, allow $\beta_n$ to depend on $n$. In particular, we allow $\beta_n = o(1)$. Let $c^{SN}(\beta_n)$ be the SN critical value with size $\beta_n$ and define the set $\hat{J}_{SN} \subset \{1,\dots,p\}$ by 
\begin{equation}
	\label{eq:J-SN}
	\hat{J}_{SN} := \left\{j \in \{1,\dots, p\}: \sqrt{n}\hat{\mu}_j/\sigmaH_j > -2c^{SN}(\beta_n)\right\}
\end{equation}

Let $\hat{k} = |\hat{J}_{SN}|$. Then, the two step SN critical value is defined by 
\begin{equation}
	\label{eq:cSN-2S}
	c^{SN, 2S}(\alpha) = \begin{cases} 
	\frac{\Phi^{-1}(1 - (\alpha - 2\beta_n)/\hat{k})}{\sqrt{1 - \Phi^{-1}(1 - (\alpha - 2\beta_n)/\hat{k})}}, & \text{ if }\hat{k} \geq 1 \\
	0, &\text{ if }\hat{k} = 0
	 \end{cases}
\end{equation}
Then paper claims the following theorem 
\begin{theorem}
	\label{thm:2SSN-validity}
	Suppose there exist constants $0 < c_1 < 1/2$ and $C_1 > 0$ such that  
	\begin{align*}
	    	M_{n,3}^3 \log^{3/2}\left(\frac{p}{\beta_n \wedge (\alpha - 2\beta_n)}\right) & \leq C_1 n^{1/2 - c_1} \\ 
	    \text{and }B_n^2\log^2(p/\beta_n) \leq C_1 n^{1/2- c_1}
	\end{align*}
	Then there exist positive constants $c, C$ depending only on $\alpha, c_1, C_1$ such that under $H_0$, 
	\begin{equation}
		\mathbb{P}(T > c^{SN, 2S}(\alpha)) \leq \alpha + Cn^{-c}
	\end{equation}
	Moreover, this bound holds uniformly over all distribution $\mathscr{L}_X$ satisfying (6) and the above condition. In addition, if all components of $X_1$ are independent, $\mu_j = 0$ and $p = p_n \rightarrow \infty$ while $\beta_n \rightarrow 0$ then 
	\[\mathbb{P}(T > c^{SN, 2S}(\alpha)) \rightarrow 1- e^{-\alpha}\]
\end{theorem}

\subsubsection{Bootstrap Methods}

Section considers Multiplier Bootstrap and Empitical Bootstrap methods. These methods are computationally harder but they lead to less conservative tests.

\paragraph{One-Step Method}

First consider the one-step method (without moment selection). In order to make the test have size $\alpha$, it is enough to choose the critical value as a bound on the $(1-\alpha)$ quantile of the distribution of 
\[\max_{1\leq j \leq p} \sqrt{n}(\muH-\mu_j)/\sigmaH_j\]
The self normalizing method finds such a bound using the union bound and moderate deviation inequality for self-normalized sums. However, SN method may be conservative as it ignores correlation between the coordinates in $X_i$. 

Alternatively, we consider a Gaussian approximation. Under suitable regularity conditions
\[\max_{i\leq j\leq p} \sqrt{n}(\muH - \mu_j)/\sigmaH_j \approx \max_{1\leq j \leq p} \sqrt{n}(\muH_j - \mu_j)/\sigma_j = \max_{1 \leq j \leq p} \sqrt{n}\mathbb{E}_n[Z_{ij}]\]
where $Z_i = (Z_{i1},\dots, Z_{ip})^T$ are defined above $(Z_j = (X_j - \mu_j)/\sigma_j)$. When $p$ is fixed, the central limit theorem shows that, as $n \rightarrow \infty$, 
\[\sqrt{n}\mathbb{E}_n[Z_i] \wcov Y, \text{ with }Y=(Y_1, \dots, Y_p)^Y \sim N(0, \E[Z_1Z_1^T])\] 
By the continuous mapping theorem, this gives us that 
\[\max_{1 \leq j \leq p}\sqrt{n}\mathbb{E}_n[Z_{ij}]\wcov \max_{1\leq j \leq p} Y_j\]
so we can take the critical value to be the $(1-\alpha)$ wuantile of $\max_{1\leq j \leq p} Y_j$. This theory does not cover when $p$ grows with $n$. Different tools should be used to derive an appropriate critical value for the test. A possible approach is to use a Berry-Esseen theorem that provides a suitable non-asymptotic bound between the distributions of $\sqrt{n}\mathbb{E}_n[Z_i]$ and $Y$. However, such Berry Esseen bounds require $p$ to be small in comparasion with $n$ in order to garuntee that the distribution of $\sqrt{n} \mathbb{E}_n Z_i$ is similar to that of $Y$. This approach builds on the work of (Chernozhukov, Chetverikov, and Kato, 2013, 2017) to show that, under some mild regularity conditions, the distribution of $\max_{1 \leq j \leq p} \sqrt{n}\mathbb{E}_n[Z_{ij}]$ can be approximated by that of $\max_{1 \leq j \leq p}$ in the sense of Kolmogrov distance even when $p$ is larger or much larger than $n$. 

Still, the distribution of $\max_{1\leq j \leq p} Y_j$ is typically unknown because the covariance structure of $Y$ is unknown. So we will approximate the distribution of $\max_{1\leq j \leq p}Y_j$ by one of the following two bootstrap procedures:

\textbf{Algorithm} (Multiplier bootstrap)
\begin{enumerate}
	\item Generate independent standard normal variables $\epsilon_1, \dots, \epsilon_n$ independent of the data 
	\item Construct the multiplier bootstrap test statistic 
	\begin{equation}
		\label{eq:CCK-30}
		W^{MB} = \max_{1 \leq j \leq p} \frac{\sqrt{n}\mathbb{E}_n[\epsilon_i(X_{ij} - \muH_j)]}{\sigmaH_j} 
	\end{equation}
	\item Calculate $c^{MB}(\alpha)$ as the conditional $(1-\alpha)$-quantile of $W^{MB}$ given $X_1^n$
\end{enumerate}

\textbf{Algorithm} (Empirical bootstrap)
\begin{enumerate}
	\item Generate a bootstrap sample $X_1^*, \dots X_n^*$
	\item Construct the empirical bootstrap test statistic 
	\begin{equation}
		\label{eq:CCK-31}
		W^{EB} = \max_{1\leq j \leq p} \frac{\sqrt{n}\mathbb{E}_n[X_{ij}^* - \muH_j]}{\sigmaH_j}
	\end{equation}
	\item Calculate $c^{EB}(\alpha)$ as the contional $(1-\alpha)$ quantile of $W^{EB}$ given $X_1^n$.
\end{enumerate}
We call these the one step multipler bootstrap and empirical bootsrap critical values, respectively, with size $\alpha$. Can be computed with any precision using simulation. 

Intuitively it is expected that the multiplier bootstrap works well since, conditional on the data, the vector 
\[\left(\frac{\sqrt{n}\mathbb{E}[\epsilon_i(x_{ij} - \muH_j)]}{\sigma_j}\right)_{1\leq j \leq p}\]
has the centered normal distribution with covariance matrix 
\begin{equation}
	\label{eq:2.34}
	\mathbb{E}_n\left[\frac{(X_{ij} - \muH_j)}{\sigmaH_j}\frac{(X_{ik} - \muH_k)}{\sigmaH_k}\right], 1 \leq j, k\leq p
\end{equation}
which should be close to the covariance matrix of the vector $Y$. Indeed by Theorem 2 in Chenozhukov, Chetverikov, and Kato (2015), the primary factor for the bound on the Kolmogorov
\footnote{The Kolmogorov Distance is defined as, for two pr. measures $\mu,\nu$ on $\SR$, $\text{Kolm}(\mu,\nu) := \sup_{x\in \SR}\left|\mu\left((-\infty, x]\right) - \nu\left((-\infty, x]\right)   \right|$}
distance between the conditional distribution of $W$ and the distribution of $\max_{1\leq j \leq p} Y_j$ is 
\[\max_{1\leq j, k \leq p} \left|\mathbb{E}_n\left[\frac{(X_{ij} - \muH_j)}{\sigmaH_j}\frac{(X_{ik} - \muH_k)}{\sigmaH_k}\right] - \mathbb{E}[Z_{1j}Z_{1k}] \right|\]
which is shown to be small even when $p \gg n$ (under suitable conditions).

Following theorem establishes validity of the MB and EB critical values. 
\vbox{ }
\begin{theorem}[Validity of one-step MB and EB methods]
	\label{thm:CCK-4.3}
	Let $c^B(\alpha)$ stand for either $c^{MB}(\alpha)$ or $c^{EB}(\alpha)$. Suppose that there exist constants $0 < c_1 < 1/2$ and $C_1 > 0$ such that 
	\begin{equation}
		\label{eq:CCK-35}
		(M_{n,3}^3 \vee M_{n,4}^2 \vee B_n)^2 \log^{7/2}(pn) \leq C_1 n^{1/2 - c_1}
	\end{equation}
	Then there exist positive constants $c, C$ depending only on $c_1, C_1$ such that, under $H_0$,
	\begin{equation}
		\label{eq:CCK-36}
		\mathbb{P}(T < c^B(\alpha))\leq \alpha + Cn^{-c}
	\end{equation}
	In addition, if $\mu_j = 0$ for all $j$, then 
	\begin{equation}
		\label{eq:CCK-37}
		\left|\mathbb{P}(T > c^B(\alpha)) - \alpha \right| \leq Cn^{-c}
	\end{equation}
	Moreover both bounds hold uniformly over all distributions $L_X$ satisfying the conditions (\ref{eq:CCK-12}) and (\ref{eq:CCK-35}).
\end{theorem}

Leave analysis of more general exchangeable weighted bootstraps in the high dimensional setting for future works. Also observe that the condition $(\ref{eq:CCK-35})$ required fof the validity of the one-step MB/EB methods is stronger than qhat is required for validity of the two-step $SN$ method.

\paragraph{Two-step Methods} 

Now consider combining bootstrap methods with inequality selection. To describe, let $0 < \beta_n < \alpha/2$ be some constant. As before, $\beta_n$ can depend on $n$. Let $c^{MB}(\beta_n)$ and $c^{EB}(\beta_n)$ be one-step MB and EB critical values with size $\beta_n$, repsectively. Define the sets $\hat{J}_{MB}$ and $\hat{J}_{EB}$ by 
\[\hat{J}_B := \left\{j \in \{1,\dots, p\}: \sqrt{n}\muH_j/\sigmaH_j > -2c^B(\beta_n)\right\}\]
Then, the two-step MB and EB critical values $c^{MB,2S}(\alpha)$ and $c^{EB, 2S}(\alpha)$ are defined by the following procedures

\textbf{Algorithm} (Multiplier bootstrap with inequality selection).
\begin{enumerate}
	\item Generate independent standard normal random variables $\epsilon_1, \dots,\epsilon_n$ independent of the data $X_{1}^n$.
	\item Construct the multiplier bootstrap test statistic 
	\[W_{\hat{J}_{MB}} = \begin{cases}
	\max_{j \in \hat{J}_{MB}} \frac{\sqrt{n}\mathbb{E}_n[\epsilon_n(X_{ij} - \muH_j)]}{\sigmaH_j} & \text{ if $\hat{J}_{MB}$ is not empty} \\
	0 & \text{ otherwise}	
	\end{cases}\]
	\item Calculate $c^{MB, 2S}$ as the conditional $(1-\alpha + 2\beta_n)$-quantile of $W_{\hat{J}_{MB}}$ given the data
\end{enumerate}

\textbf{Algorithm} (Empirical bootstrap with inequality selection).
\begin{enumerate}
	\item Generate a bootstrap sample $X_1^*, \dots, X_n^*$ as i.i.d draws from the empirical distribution of $X_1^n = \{X_1, \dots, X_n\}$.
	\item Construct the empirical bootstrap test statistic 
	\[W_{\hat{J}_{EB}} = \begin{cases}
	\max_{j \in \hat{J}_{EB}} \frac{\sqrt{n}\mathbb{E}_n[X_{ij}^* - \muH_j]}{\sigmaH_j} & \text{ if $\hat{J}_{EB}$ is not empty} \\
	0 & \text{ otherwise}	
	\end{cases}\]
	\item Calculate $c^{EB,2S}(\alpha)$ as the conditional $(1-\alpha + 2\beta_n)$-quantile of $W_{\hat{J}_{EB}}$ given the data
\end{enumerate}

\begin{theorem}[Validity of two-step MB and EB methods]
	\label{thm:CCK-4.4}
	Let $c^{B,2S}(\alpha)$ stand for either $c^{MB,2S}(\alpha)$	or $c^{EB,2S}(\alpha)$. Suppose that the asusmption of Theorem $\ref{thm:CCK-4.3}$ is satisfied. Moreover, suppose that $\log(1/\beta_n) \leq C_1 \log n$. Then there exist positive constants $c,C$ depending only on $c_1,C_1$ such that under $H_0$,
	\[\mathbb{P}(T > c^{B,2S}(\alpha)) \leq \alpha + Cn^{-c}\]
	In addition, if $\mu_j = 0$ for all $1 \leq j \leq p$, then 
	\[\mathbb{P}(T > c^{B, 2S}(\alpha)) \geq \alpha - 3 \beta_n - Cn^{-c}\]
	so that under an extra assumption that $\beta \leq C_1 n^{-c_1}$\footnote{which is to say $\beta_n$ goes to 0 reasonable fast} 
	\[\left|\mathbb{P}(T> c^{B,2S}(\alpha)) - \alpha \right| \leq Cn^{-c}\]
	Moreover all these bounds hold uniformly over all distributions $L_X$ satisfying (\ref{eq:CCK-12}) and (\ref{eq:CCK-35})
\end{theorem}

It is sort of interesting to note that all these theorems are ``non-asymptotic'' in the sense that if the conditions hold then these inequalities ``really'' hold.

\subsubsection{Hybrid Methods}

Have considered one-step SN, MB, and EB methods and their two-step variatns. In fact, can also consider hybrids of these methods. For example, can use the SN method for inquality selection and then apply the MB or EB method for the selected inequalities, which is computationally more tractable. Notate this as the HB method. Formally, let $0 < \beta_n < \alpha/2$ be some constants and recall the set $\hat{J}_{SN}\subset \{1,\dots,p\}$ defined above. Then the hybrid MB critical value, $c^{MB,H}(\alpha)$ is defined by the following procudure:

\textbf{Algorithm} (Multiplier Bootstrap Hybrid method).
\begin{enumerate}
	\item Generate independent standard normal random variables $\epsilon_1,\dots, \epsilon_n$ independent of the data $X_1^n$.
	\item Construct the bootstrap test statistic: 
	\[W_{\hat{J}_{SN}} = \begin{cases}
	\max_{j \in \hat{J}_{SN}} \frac{\sqrt{n}\mathbb{E}_n[\epsilon_n(X_{ij} - \muH_j)]}{\sigmaH_j} & \text{ if $\hat{J}_{SN}$ is not empty} \\
	0 & \text{ otherwise}	
	\end{cases}\]
	\item Calvculate $c^{MB,H}(\alpha)$ as the contional $(1- \alpha + 2\beta_n)$-quantile of $W_{\hat{J}_{SN}}$ given the data.
\end{enumerate}

This can be equibalently defined for the empirical bootstrap.

\begin{theorem}[Validity of hybrid two-step methods]
Let $c^{MB,H}$ stand either for $c^{MB,H}(\alpha)$ or $c^{MB,H}(\alpha)$. Suppose that there exist constants $0<c_1<1/2$ and $C_1 > 0$ such that (\ref{eq:CCK-35}) is verified. Moreover, suppose that $\log(1/\beta_n)\leq C_1 \log n$. Then all the conclusions of Theorem \ref{thm:CCK-4.4} hold wilth $c^{B,MS}(\alpha)$ replaced by $c^{B,H}(\alpha)$. 
\end{theorem}

\subsubsection{Three-step method} 

In empirical studies based on moment inqualites one generally has inequalities of the form 
\begin{equation}
	\label{eq:CCK-41}
	\E[g_j(\xi, \theta)]\leq 0\hbox{ }\hbox{ }\hbox{ }\hbox{ }\text{ for all }j = 1,\dots,p
\end{equation}
where $\xi$ is a vector of r.v's from a distribution denoted $\calL_\xi$, $\theta = (\theta_1, \dots, \theta_r)^T$ is a vector of parameters in $\SR^r$ and $g_1,\dots,g_p$ a set of (known) functions. In these studies, inequalities (\ref{eq:CCK-1}) and (\ref{eq:CCK-2}) arise when one tests the null hypothesis $\theta = \theta_0$ against the alternative $\theta \neq \theta_0$ on the i.i.d data $\xi_1,\dots \xi_n$ by setting $X_{ij} := g_j(\xi_i,\theta_0)$ and $\mu_j := \E[X_{1j}]$. So far, have shown how to increase the poser of such tests by employing inequality selection procedures that allow the researcher to drop uninformative inequalities. In this subsection, combine this selection procedure with another procedure suitable for the model (\ref{eq:CCK-41}) by dropping \textit{weakly informative} inequalities, that is inequalities $j$ with the function $\theta \mapsto \E[g_j(\xi,\theta)]$ being flat or nearly flat around $\theta = \theta_0$. 

When the tested value $\theta_0$ is close to some $\theta$ satisfying (\ref{eq:CCK-41}), such inequalities can only provide a weak signal of violation of the hypothesis $\theta = \theta_0$ in the sense that they have $\mu_j \approx 0$ and so it is useful to drop them. For brevity, only consider weakly informative inequality selection based on the MB and EB methods and note that similar results can be obtained for the self-normalized method. Also only consider the case where the function $\theta \mapsto g_j(\xi,\theta)$ are almost everywhere continuously differentiable and leave the extension to non-differentiable functions to future work. 

Start with the necessary notation. Let $\xi_1, \dots, \xi_n$ be a sample of observations from the distribution of $\xi$. Suppose that we are interested in testing the null hypothesis and alternative hypothesis
\begin{align*}
    H_0: \E[g_j(\xi,\theta_0)] &\leq 0 \hbox{ }\hbox{ }\text{ for all }j = 1,\dots,p \\
    H_a: \E[g_j(\xi,\theta_0)] &> 0 \hbox{ }\hbox{ }\text{ for some }j = 1,\dots,p 
\end{align*}
where $\theta_0$ is some value of the parameter $\theta$. Define 
\begin{align*}
    m_j(\xi,\theta) &:= \left(m_{j1}(\xi,\theta),\dots,m_{jr}(\xi,\theta)\right)^T \\
                    &:= \left(\pderiv{g_j(\xi,\theta)}{\theta_1}, \dots, \pderiv{g_j(\xi,\theta)}{\theta_r}\right)^T
\end{align*}
Further, let $X_{ij} := g_j(\xi_i,\theta_0), \mu_j := \E[X_{1j}], \sigma_j := \left(\Var(X_{1j}) \right)^{1/2}, V_{ijl} := m_jl(\xi_i,\theta_0), \mu_{jl}^B = \E[V_{1jl}]$, and $\sigma_{jl}^V := \left(\Var(V_{1jl})\right)^{1/2}$. Assume that
\begin{align}
	\label{eq:CCK-42}
	\E[X_{1,j}^2] &< \infty, \sigma_j > 0, j = 1,\dots, p\\
	\label{eq:CCK-43}
	\E[X_{1,j,l}^2] &< \infty, \sigma_{jl}^V >0, j = 1,\dots,p, l = 1,\dots, r
\end{align}
In addition, let $\muH_j = \mathbb{E}_n[X_{ij}]$ and $\sigmaH_j = \left(\mathbb{E}\left[(X_{ij} -\muH_j)^2\right]\right)^{1/2}$ be estimators of $\mu_j$ and $\sigma_j$, respectively. Similarly let  $\muH_{jl}^V = \mathbb{E}_n[V_{ijl}]$ and  $\sigmaH_{jl}^V = \left(\mathbb{E}\left[(V_{ij} -\muH_{jl}^V)^2\right]\right)^{1/2}$ be esttimators of $\mu_{jl}^V$. The inequality selection derived is similar to the bootstrap methods described in Section 4

\textbf{Algorithm}(Multiplier bootstrap for gradient statistic).
\begin{enumerate}
	\item Generate independent standard normal variables $\epsilon_1, \dots, \epsilon_n$ independent of the data. 
	\item Construct the multiplier bootstrap gradient statistic 
	\begin{equation}
		\label{eq:CCK-44} 
		W_{MB}^V = \max_{j,l} \frac{\sqrt{n}|\mathbb{E}_n[\epsilon_i(V_{ijl}-\muH_{jl}^V)]|}{\sigmaH_{jl}^V}
	\end{equation}
	\item For $\gamma \in (0,1)$, calculate $c^{MB,V}(\gamma)$ as the conditional $(1-\gamma)$ quantile of $W_{MB}^V$ given the data.
\end{enumerate}

\textbf{Algorithm}(Empirical bootstrap for gradient statistic).
\begin{enumerate}
	\item Generate a bootstrap sample $V_1^*, \dots, V_n^*$ as i.i.d draws from the data
	\item Construct the empirical bootstrap gradient statistic 
	\begin{equation}
		\label{eq:CCK-45} 
		W_{EB}^V = \max_{j,l} \frac{\sqrt{n}|\mathbb{E}_n[V_{ijl}^*-\muH_{jl}^V]|}{\sigmaH_{jl}^V}
	\end{equation}
	\item For $\gamma \in (0,1)$, calculate $c^{EB,V}(\gamma)$ as the conditional $(1-\gamma)$ quantile of $W_{EB}^V$ given the data.
\end{enumerate}

For $c_2, C_2 > 0$, let $\varphi_n$ be a sequence of constants satisfying $\varphi_n\log n \geq c_2$ and let $\beta_n$ be a sequence of constants satisfying $0< \beta_n < \alpha/4$ and $\log\left(1/(\beta_n - \varphi_n)\right) \leq C_2\log n$ where $\alpha$ is the nominal level of the test. Define three estimated sets of inequalities 
\begin{align*}
    \hat{J}_B &:= \left\{j \in \{1,\dots, p\}: \sqrt{n}\muH_j/\sigmaH_j > -2c^B(\beta_n)\right\} \\
    \hat{J}_B' &:= \left\{j \in \{1,\dots, p\}: \sqrt{n}|\muH_{jl}^V/\sigmaH_{jl}^V| > 3c^{B,V}(\beta_n - \phi_n)\text{ for some }l= 1,\dots,r\right\} \\
    \hat{J}_B'' &:= \left\{j \in \{1,\dots, p\}: \sqrt{n}|\muH_{jl}^V/\sigmaH_{jl}^V| > c^{B,V}(\beta_n + \phi_n)\text{ for some }l= 1,\dots,r\right\} 
\end{align*}
where $B$ stands for either $MB$ or $EB$.

The derived weakly informative inequality selection procedure requires that both the test statistic and the critical value depend on the estimated sets of inequalities. Let $T^B$ and $c^{B,3S}$ denote the test statistic and the critical value. If the set $\hat{J}_B'$ is empty, set the test statistic and critical value $T^B = c^{B,3S} = 0$. Otherwise, define the test statistic 
\[T^B = \max_{j\in \hat{J}_B'}\frac{\sqrt{n}\muH_j}{\sigmaH_j}\]
and define the three-step MB/EB critical values $c^{B,3S}(\alpha)$ for the test by the same bootstrap procedures as those for $c^{B,2S}(\alpha)$ with $\hat{J}_B$ replaced by $\hat{J}'\cap\hat{J}_B''$ and also $2\beta_n$ replaced by $4\beta_n$. That is $c^{B,3S}(\alpha)$ is the conditional $(1-\alpha + 4\beta_n)$-quantile of $W_{\hat{J}_B'\cap\hat{J}_B''}$ given the data. 

Stating the main results of this section requires the following notation. Let 
\[Z_{ijl}^V := (V_{ijl} - \mu_{ijl}^V)/\sigma_{jl}^V\hbox{ }\text{ and }\hbox{ }M_{n,k}^V := \max_{j,l} \left(\E[|Z_{ijl}^V|^k]\right)^{1/k}\hbox{ }\text{ and }\hbox{ }B_n^V :=\left(\E[\max_{j,l} (Z_{ijl}^V)^4\right)^{1/4}\]
\begin{theorem}[Validity of three-step MB and EB methods]\footnote{If I understand correctly, this method only selects out weakly uninformative inequalites based on the gradient, not necessarily inequalities with say, $\muH_j \ll 0$.}
	\label{thm:CCK-4.6}
	Let $T^B$ and $c^{B,3S}(\alpha)$ stand for $T^{MB}$ and $c^{MB,3S}(\alpha)$ or for $T^{EB}$ and $c^{EB,3S}(\alpha)$. Suppose there exist constants $0 < c_1 < 1/2$ and $C_1 > 0$ such that 
	\begin{equation}
		\label{eq:CCK-48}
		\left(M_{n,3}^3 \vee M_{n,4}^2 \vee B_n)^2 \log^{7/2}(pn)\right) \leq C_1 n^{1/2 - c_1}
	\end{equation}
	and 
	\begin{equation}
		\label{eq:CCK-49}
		\left((M_{n,3}^V)^3 \vee (M_{n,4}^V)^2 \vee (B_n^V)^2 \log^{7/2}(pn)\right) \leq C_1 n^{1/2 - c_1}
	\end{equation}
	Moreover, suppose that $\log(1/(\beta_n -\phi_n)) \leq C_2 \log n$ and $\phi_n \log n \geq c_2$ for some constants $c_2, C_2 > 0$. Then there exist positive constants $c,C$ depending only on $c_1, C_1, c_2, C_2$such that, under $H_0$,
	\[\mathbb{P}(T^B > c^{B,3S}(\alpha))\leq \alpha + Cn^{-c}\]
	and this bound holds uniformly over all distributions $L_\xi$ satisfying (\ref{eq:CCK-42}),(\ref{eq:CCK-43}),(\ref{eq:CCK-48}), and (\ref{eq:CCK-49}).
\end{theorem}

\subsection{Power}

Consider the same general setup as described in the intruction and assume that (\ref{eq:CCK-12}) holds. Pick any $\alpha \in (0,1/2)$ and consider the test of the form 
\[T > \hat{c}(\alpha) \implies \text{ reject }H_0\]
Where $\hat{c}(\alpha)$ is equal to $c^{SN}(\alpha), c^{SN,2S}(\alpha), c^{MB}(\alpha), c^{MB,2S}(\alpha), c^{EB}(\alpha), c^{EB,2S}(\alpha), c^{MB,H}(\alpha)$, or $c^{EB,H}(\alpha).$\footnote{Importantly, the three step procedure is not included here.} The following result holds:
\begin{theorem}[Rate of uniform consistency]
	\label{thm:CCK-5.1}
	Suppose there exist constants $0 < c_1 < 1/2$ and $C_1 > 0$ such that 
	\begin{equation}
		\label{eq:CCK-50}
		M_{n,4}^2 \log^{1/2}p \leq C_1 n^{1/2 - c_1}\text{ and }\log^{3/2}p \leq C_1 n
	\end{equation}
	In addition, suppose that $\inf_{n\geq 1}(\alpha - 2\beta_n)\geq c_1 \alpha$ whenever inequality selection is used. Then there exist constants $c,C > 0$ depending only on $\alpha, c_1, C_1$ such that for every $\epsilon \in (0,1)$, whenever\footnote{Seems similar to LASSO condition}
	\[\max_{1\leq j \leq p} \mu_j/\sigma_j \geq (1 + \epsilon + C\log^{-1/2} p)\sqrt{\frac{2\log(p/\alpha)}{n}}\]
	we have 
	\[\mathbb{P}(T > \hat{c}(\alpha)) \geq 1 - \frac{C}{\epsilon^2 \log(p/\alpha)} - Cn^{-c}\]
	Therefore, when $p = p_n \rightarrow \infty$, for any sequence $\epsilon_n$ satisfying $\epsilon_n \rightarrow 0$ and $\epsilon_n\sqrt{\log p_n}\rightarrow\infty$, as $n \rightarrow \infty$, we have 
	\begin{equation}
		\label{eq:CCK-51}
		\inf_{\mu\in\calB_n} \mathbb{P}_\mu(T > \hat{c}(\alpha)) \geq 1 - o(1)
	\end{equation}
	where 
	\[\calB_n = \left\{\mu = (\mu_1, \dots, \mu_p): \max_{1 \leq j \leq p} \mu_j/\sigma_j \geq \bar{r}_n (1+ \epsilon_n)\sqrt{2\log(p_n)/n}\right\}\]
	and $P_\mu$ denotes the probability measure for the distribution $\calL_X$ having mean $\mu$. Moreover, the above asymptotic result (\ref{eq:CCK-51}) holds uniformly with respect to any sequnce of distributions $\calL_X$ satisfying (\ref{eq:CCK-12}) and (\ref{eq:CCK-50}).
\end{theorem}
Theorem \ref{thm:CCK-5.1} shows that tests are uniformly consistent against all alternatives excluding those in a small neighborhood of alternatives that are too close to the null. Size of this neighborhood is shrikning at a fast rate. They show in a working paper that no test can be consistent against all alternatives whose distance from the null converges to zero faster than $\sqrt{(\log p_n)/n}$ and that the tests above are minimax optimal.

\subsection{Monte Carlo Experiments}

Results are given, I would check document. This likely concludes my notes on this topic. 
