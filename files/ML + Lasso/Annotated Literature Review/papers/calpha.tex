\section{C-alpha Tests and Their Use {\small Jerzy Neyman, (IJS, 1979)}}


This paper can be found at the following stable JSTOR \href{https://www.jstor.org/stable/25050174}{link}. It appeared in the Indian Journal of Statistics in 1979.  

\subsection{General Idea of C-alpha Tests}%
\label{subsec:neyman-2}

Consider a sequence of independent and identically distributed random variables \(X_n(\xi,\theta)\), possible vectors with density \(X_n(\xi,\theta) \sim p(x|\xi,\theta)\) depending on two parameters: \(\xi\)--scalar and \(\theta\)--possibly a vector of size \(x \geq 1\). We are only interested in testing the hypothesis: \(H_0:\xi=0\), the parameter \(\theta\) is a nuisance parameter.

Assume that \(\theta \in \Theta \in \tau\), that is  \(\Theta\) is an open set. The following discussion presupposes certain properties of regularity of \(p(x|\xi,\theta)\), enough that you can take derivatives and second derivatives under the integral. The sample space of \(X\) is denoted \(W\). 

In order to define a test of class \(C(\alpha)\), consider an arbitrarily measurable function  \(f(x)\) defined for all  \(x\in W\) and that for \(\xi=0\) the random variable \(f[X(0,\theta)]\) has finite variance \(\sigma^2(\theta)\). Let \(f_1(\theta)\) be the expectation of \(f[X(0,\theta)]\). Then, by CLT the function
\begin{equation}
	\label{eq:neyman-1}
	Z_n(\theta) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \frac{f[X_i(0,\theta)]-f_1(\theta)}{\sigma(\theta)}  \wcov N(0,1)
\end{equation}

So for large \(n\), if we knew \(\theta\), we could use the test statistic  \(Z_n(\theta)\), comparing this to quantiles of the  Normal distribution to get rejection regions. The problem is that we have to estimate \(\theta\). So the question becomes: what is the asymptotic distribution of \(Z_n(\hat \theta)\).

Impose the limitation that \(\hat\theta\) is ``locally-\(\sqrt{n}\) consistent". That is, for each possible \(\xi\):
\begin{equation}
	\label{eq:neyman-3}
	\sqrt{n}\left|\hat\theta-\theta-A\xi\right| = O_p(1)
\end{equation}
where \(A \in \SR\) is just a constant and \(A\xi\) is the bias in the estimate of \(\hat\theta\). If \(A=0\), there is no bias and the estimator \(\hat\theta\) is labelled ``consistent in the large", otherwise it is only ``locally" consistent.

In the case that \(\xi=0\), one of the basic theorems of Neyman (1959) indicates that for \(Z_n(\hat\theta)\) to have the same asymptotic distribution of  \(Z_n(\theta)\) it is necessary and sufficient that the function  \(f\) be orthogonal to all the logarithmic derivatives. That is 
\begin{equation}
	\label{eq:neyman-4}
	\phi_j(x,\theta) = \frac{\partial \log p}{\partial \theta_j}\large|_{\xi=0} 
\end{equation}

Starting with an arbitrary function \(f\), it is easy to replace it by one orthogonal to the \(\phi_j\), name 
 \begin{equation}
	\label{eq:neyman-5}
	g(x,\theta) = f(x,\theta)-\sum_{j=1}^s \alpha_j\phi_j(x,\theta)
\end{equation}
where \(f(x,\theta) = f(x)-f_1(x)\) and
