\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Generalized Random Forests; \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Susan Athey, Julie Tibshirani, Setfan Wager (AOS, 2018)}}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction}{2}{subsection.1.1}\protected@file@percent }
\newlabel{MC1}{{2}{2}{Introduction}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Related Work}{2}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Generalized Random Forests}{3}{subsection.1.2}\protected@file@percent }
\newlabel{eq:mc_sample}{{3}{3}{Generalized Random Forests}{equation.1.3}{}}
\newlabel{eq:alpha_bi}{{4}{3}{Generalized Random Forests}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Splitting to Maximize Heterogeneity}{3}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{eq:theta_nu_sample}{{5}{4}{Splitting to Maximize Heterogeneity}{equation.1.5}{}}
\newlabel{prop1}{{1}{4}{}{prop.1.1}{}}
\newlabel{eq:delta_criterion}{{6}{4}{}{equation.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}The Gradient Tree Algorithm}{4}{subsubsection.1.2.2}\protected@file@percent }
\newlabel{eq:thetaT_def}{{7}{4}{The Gradient Tree Algorithm}{equation.1.7}{}}
\newlabel{eq:Ap_def}{{8}{5}{The Gradient Tree Algorithm}{equation.1.8}{}}
\newlabel{eq:labeling_step}{{9}{5}{The Gradient Tree Algorithm}{equation.1.9}{}}
\newlabel{eq:regression_step_criterion}{{10}{5}{The Gradient Tree Algorithm}{equation.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Algorithms for growing generalized random forests}}{5}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Algorithm 1}}}{5}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Algorithm 2}}}{5}{figure.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Asymptotic Analysis}{6}{subsection.1.3}\protected@file@percent }
\newlabel{eq:expected_score}{{11}{6}{Asymptotic Analysis}{equation.1.11}{}}
\newlabel{as:lipschitz}{{1}{6}{}{assumption.1.1}{}}
\newlabel{as:smooth_id}{{2}{6}{}{assumption.1.2}{}}
\newlabel{as:lipschitz_variogram}{{3}{6}{}{assumption.1.3}{}}
\newlabel{as:psi_regularity}{{4}{6}{}{assumption.1.4}{}}
\newlabel{as:existence}{{5}{7}{}{assumption.1.5}{}}
\newlabel{as:convexity}{{6}{7}{}{assumption.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}A Central Limit Theorem for Generalized Random Forests}{7}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{eq:thetaHat_appx}{{12}{7}{A Central Limit Theorem for Generalized Random Forests}{equation.1.12}{}}
\newlabel{thm:consistency}{{1}{8}{}{theorem.1.1}{}}
\newlabel{eq:betaMin}{{13}{8}{A Central Limit Theorem for Generalized Random Forests}{equation.1.13}{}}
\newlabel{thm:main-result}{{1}{8}{}{lemma.1.1}{}}
\newlabel{thm:clt}{{2}{8}{}{theorem.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Confidence Intervals via the Delta Method}{8}{subsection.1.4}\protected@file@percent }
\newlabel{eq:varThetaT*}{{15}{9}{Confidence Intervals via the Delta Method}{equation.1.15}{}}
\newlabel{eq:sigmaHat}{{16}{9}{Confidence Intervals via the Delta Method}{equation.1.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Consistency of the Bootstrap of Little Bags}{9}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Learning in NPR \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Benedikt Bauer and Michael Kohler (AOS, 2019)}}{10}{section.2}\protected@file@percent }
\newlabel{sec:dl_npr}{{2}{10}{Deep Learning in NPR \textit {\small Benedikt Bauer and Michael Kohler (AOS, 2019)}}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction}{10}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Rate of Convergence}{10}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{def:DLNPR-1}{{1}{10}{}{definition.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Curse of dimensionality}{11}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{def:DLNPR-2}{{2}{11}{}{definition.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Neural Networks}{11}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq:DLNPR-1}{{1}{12}{Neural Networks}{equation.2.1}{}}
\newlabel{eq:DLNPR-2}{{2}{12}{Neural Networks}{equation.2.2}{}}
\newlabel{eq:DLNPR-3}{{3}{12}{Neural Networks}{equation.2.3}{}}
\newlabel{eq:DLNPR-4}{{4}{12}{Neural Networks}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Main Results}{13}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Notation}{13}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{13}{subsection.2.2}\protected@file@percent }
\newlabel{eq;DLNPR-5}{{{5}}{14}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{AMS.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A not completely connected neural network $f:\mathbb  {R}^5\rightarrow \mathbb  {R}$ from $\mathcal  {F}_{M^*,d^*,d,\alpha }^{\text  {(neural networks)}}$ with the structure $f(x) = \sum _{i=1}^3 \mu _i \cdot \sigma (\sum _{j=1}^4 \lambda _{i,j}\cdot \sigma (\sum _{v=1}^5 \theta _{i,j,v} \cdot x^{(v)}))$ (all weights with an index including zero neglected for a clear illustration). [Lifted from the paper]}}{14}{figure.2.1}\protected@file@percent }
\newlabel{fig:DLNPR-1}{{1}{14}{A not completely connected neural network $f:\SR ^5\rightarrow \SR $ from $\calF _{M^*,d^*,d,\alpha }^{\text {(neural networks)}}$ with the structure $f(x) = \sum _{i=1}^3 \mu _i \cdot \sigma (\sum _{j=1}^4 \lambda _{i,j}\cdot \sigma (\sum _{v=1}^5 \theta _{i,j,v} \cdot x^{(v)}))$ (all weights with an index including zero neglected for a clear illustration). [Lifted from the paper]}{figure.2.1}{}}
\newlabel{eq:DLNPR-6}{{6}{14}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of the components of a function from $\mathcal  {H}^{(l)}$ [Lifted from the paper]}}{15}{figure.2.2}\protected@file@percent }
\newlabel{fig:DLNPR-Fig2}{{2}{15}{Illustration of the components of a function from $\calH ^{(l)}$ [Lifted from the paper]}{figure.2.2}{}}
\newlabel{eq:DLNPR-7}{{7}{15}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.7}{}}
\newlabel{eq:DLNPR-8}{{8}{15}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.8}{}}
\newlabel{eq:DLNPR-9}{{9}{15}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.9}{}}
\newlabel{def:DLNPR-3}{{3}{16}{}{definition.2.3}{}}
\newlabel{thm:DLNPR-1}{{1}{16}{Main Result}{theorem.2.1}{}}
\newlabel{eq:DLNPR-10}{{10}{16}{Main Result}{equation.2.10}{}}
\newlabel{eq:DLNPR-11}{{11}{16}{Main Result}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Application to Simulated Data}{17}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Proofs}{17}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}NPR Using Deep Neural Networks \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Johannes Schmidt-Hieber (ArXiv, 2017)}}{18}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Introduction}{18}{subsection.3.1}\protected@file@percent }
\newlabel{eq:sh-1}{{1}{18}{Introduction}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Mathematical Definition of Multilayer Neural Networks}{19}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neural Network}{19}{paragraph*.3}\protected@file@percent }
\newlabel{eq:sh-2}{{2}{19}{Neural Network}{equation.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Modeling of Deep Network Characteristics}{19}{paragraph*.4}\protected@file@percent }
\newlabel{eq:sh-3}{{3}{19}{Mathematical Modeling of Deep Network Characteristics}{equation.3.3}{}}
\newlabel{eq:sh-4}{{4}{20}{Mathematical Modeling of Deep Network Characteristics}{equation.3.4}{}}
\newlabel{eq:sh-5}{{5}{20}{Mathematical Modeling of Deep Network Characteristics}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Main Results}{21}{subsection.3.3}\protected@file@percent }
\newlabel{eq:sh-6}{{6}{21}{Main Results}{equation.3.6}{}}
\newlabel{eq:sh-7}{{7}{22}{Main Results}{equation.3.7}{}}
\newlabel{thm:sh-1}{{1}{22}{Main Result}{theorem.3.1}{}}
\newlabel{eq:sh-8}{{8}{22}{Main Result}{equation.3.8}{}}
\newlabel{eq:sh-9}{{9}{22}{Main Result}{equation.3.9}{}}
\newlabel{cor:sh-1}{{1}{22}{}{corollary.3.1}{}}
\newlabel{eq:sh-10}{{10}{22}{}{equation.3.10}{}}
\newlabel{thm:sh-2}{{2}{23}{}{theorem.3.2}{}}
\newlabel{eq:sh-11}{{11}{23}{Main Results}{equation.3.11}{}}
\newlabel{thm:sh-3}{{3}{23}{}{theorem.3.3}{}}
\newlabel{lemma:sh-1}{{1}{24}{}{lemma.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Central Limit Theorems and Bootstrap in High Dimensions \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Victor Chernozhukov, Denis Chetverikov, Kengo Kato (AoP 2017)}}{25}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Introduction}{25}{subsection.4.1}\protected@file@percent }
\newlabel{eq:cck17-1}{{1}{25}{Introduction}{equation.4.1}{}}
\newlabel{eq:cck17-2}{{2}{25}{Introduction}{equation.4.2}{}}
\newlabel{eq:cck17-3}{{3}{25}{Introduction}{equation.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Some Notation}{26}{paragraph*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}High-dimensional CLT for hyperrectangles}{26}{subsection.4.2}\protected@file@percent }
\newlabel{eq:cck17-4}{{4}{26}{High-dimensional CLT for hyperrectangles}{equation.4.4}{}}
\newlabel{eq:cck17-5}{{5}{26}{High-dimensional CLT for hyperrectangles}{equation.4.5}{}}
\newlabel{thm:ckk17-1}{{1}{27}{Abstract high-dimensional CLT for hyper-rectangles}{theorem.4.1}{}}
\newlabel{eq:cck17-6}{{6}{27}{Abstract high-dimensional CLT for hyper-rectangles}{equation.4.6}{}}
\newlabel{eq:cck17-7}{{7}{27}{Abstract high-dimensional CLT for hyper-rectangles}{equation.4.7}{}}
\newlabel{eq:cck17-8}{{8}{27}{Key Features of Theorem \ref {thm:ckk17-1}}{equation.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}High-dimensional CLT for simple and sparsely convex sets}{27}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Simple Convex Sets}{27}{subsubsection.4.3.1}\protected@file@percent }
\newlabel{eq:cck17-9}{{9}{28}{Simple Convex Sets}{equation.4.9}{}}
\newlabel{eq:cck17-10}{{10}{28}{High-dimensional CLT for simple convex sets}{equation.4.10}{}}
\newlabel{eq:cck17-11}{{11}{28}{High-dimensional CLT for simple convex sets}{equation.4.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Sparsely Convex Sets}{29}{subsubsection.4.3.2}\protected@file@percent }
\newlabel{def:cck17-3.1}{{1}{29}{Sparsely Convex Sets}{definition.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Sparse Principal Component Analysis \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Hui Zou, Trevor Hastie, Robert Tisbirani (JCGS, 2006)}}{30}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Introduction}{30}{subsection.5.1}\protected@file@percent }
\newlabel{eq:scpa-1.1}{{1}{30}{Introduction}{equation.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Motivation and Details of SPCA}{30}{subsection.5.2}\protected@file@percent }
\newlabel{eq:spca-3.1}{{2}{31}{Motivation and Details of SPCA}{equation.5.2}{}}
\newlabel{eq:spca-3.2}{{3}{31}{Motivation and Details of SPCA}{equation.5.3}{}}
\newlabel{eq:spca-3.3}{{4}{31}{Motivation and Details of SPCA}{equation.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Direct Sparse Approximation}{31}{subsubsection.5.2.1}\protected@file@percent }
\newlabel{thm:spca-1}{{1}{31}{}{theorem.5.1}{}}
\newlabel{eq:spca-3.4}{{5}{31}{}{equation.5.5}{}}
\newlabel{eq:spca-3.5}{{6}{31}{Direct Sparse Approximation}{equation.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Sparse Principal Components Based on the SPCA Criterion}{31}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{thm:spca-2}{{2}{32}{}{theorem.5.2}{}}
\newlabel{eq:spca-3.6}{{{7}}{32}{}{AMS.7}{}}
\newlabel{thm:spca-3}{{3}{32}{}{theorem.5.3}{}}
\newlabel{eq:spca-3.7}{{{8}}{32}{}{AMS.9}{}}
\newlabel{eq:spca-3.8}{{9}{32}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.9}{}}
\newlabel{eq:spca-3.9}{{10}{32}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.10}{}}
\newlabel{eq:spca-3.10}{{11}{32}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.11}{}}
\newlabel{eq:spca-3.11}{{12}{32}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.12}{}}
