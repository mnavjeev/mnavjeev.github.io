\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\zref@newlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Generalized Random Forests; \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Susan Athey, Julie Tibshirani, Setfan Wager (AOS, 2018)}}{3}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Introduction}{3}{subsection.1.1}\protected@file@percent }
\newlabel{MC1}{{2}{3}{Introduction}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Related Work}{3}{subsubsection.1.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Generalized Random Forests}{4}{subsection.1.2}\protected@file@percent }
\newlabel{eq:mc_sample}{{3}{4}{Generalized Random Forests}{equation.1.3}{}}
\newlabel{eq:alpha_bi}{{4}{4}{Generalized Random Forests}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Splitting to Maximize Heterogeneity}{4}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{eq:theta_nu_sample}{{5}{5}{Splitting to Maximize Heterogeneity}{equation.1.5}{}}
\newlabel{prop1}{{1}{5}{}{prop.1.1}{}}
\newlabel{eq:delta_criterion}{{6}{5}{}{equation.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}The Gradient Tree Algorithm}{5}{subsubsection.1.2.2}\protected@file@percent }
\newlabel{eq:thetaT_def}{{7}{5}{The Gradient Tree Algorithm}{equation.1.7}{}}
\newlabel{eq:Ap_def}{{8}{6}{The Gradient Tree Algorithm}{equation.1.8}{}}
\newlabel{eq:labeling_step}{{9}{6}{The Gradient Tree Algorithm}{equation.1.9}{}}
\newlabel{eq:regression_step_criterion}{{10}{6}{The Gradient Tree Algorithm}{equation.1.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Algorithms for growing generalized random forests}}{6}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Algorithm 1}}}{6}{figure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Algorithm 2}}}{6}{figure.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Asymptotic Analysis}{7}{subsection.1.3}\protected@file@percent }
\newlabel{eq:expected_score}{{11}{7}{Asymptotic Analysis}{equation.1.11}{}}
\newlabel{as:lipschitz}{{1}{7}{}{assumption.1.1}{}}
\newlabel{as:smooth_id}{{2}{7}{}{assumption.1.2}{}}
\newlabel{as:lipschitz_variogram}{{3}{7}{}{assumption.1.3}{}}
\newlabel{as:psi_regularity}{{4}{7}{}{assumption.1.4}{}}
\newlabel{as:existence}{{5}{8}{}{assumption.1.5}{}}
\newlabel{as:convexity}{{6}{8}{}{assumption.1.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}A Central Limit Theorem for Generalized Random Forests}{8}{subsubsection.1.3.1}\protected@file@percent }
\newlabel{eq:thetaHat_appx}{{12}{8}{A Central Limit Theorem for Generalized Random Forests}{equation.1.12}{}}
\newlabel{thm:consistency}{{1}{9}{}{theorem.1.1}{}}
\newlabel{eq:betaMin}{{13}{9}{A Central Limit Theorem for Generalized Random Forests}{equation.1.13}{}}
\newlabel{thm:main-result}{{1}{9}{}{lemma.1.1}{}}
\newlabel{thm:clt}{{2}{9}{}{theorem.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Confidence Intervals via the Delta Method}{9}{subsection.1.4}\protected@file@percent }
\newlabel{eq:varThetaT*}{{15}{10}{Confidence Intervals via the Delta Method}{equation.1.15}{}}
\newlabel{eq:sigmaHat}{{16}{10}{Confidence Intervals via the Delta Method}{equation.1.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.4.1}Consistency of the Bootstrap of Little Bags}{10}{subsubsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Deep Learning in NPR \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Benedikt Bauer and Michael Kohler (AOS, 2019)}}{11}{section.2}\protected@file@percent }
\newlabel{sec:dl_npr}{{2}{11}{Deep Learning in NPR \textit {\small Benedikt Bauer and Michael Kohler (AOS, 2019)}}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Introduction}{11}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Rate of Convergence}{11}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{def:DLNPR-1}{{1}{11}{}{definition.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Curse of dimensionality}{12}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{def:DLNPR-2}{{2}{12}{}{definition.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Neural Networks}{12}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{eq:DLNPR-1}{{1}{13}{Neural Networks}{equation.2.1}{}}
\newlabel{eq:DLNPR-2}{{2}{13}{Neural Networks}{equation.2.2}{}}
\newlabel{eq:DLNPR-3}{{3}{13}{Neural Networks}{equation.2.3}{}}
\newlabel{eq:DLNPR-4}{{4}{13}{Neural Networks}{equation.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Main Results}{14}{subsubsection.2.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Notation}{14}{subsubsection.2.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{14}{subsection.2.2}\protected@file@percent }
\newlabel{eq;DLNPR-5}{{{5}}{15}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{AMS.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A not completely connected neural network $f:\mathbb  {R}^5\rightarrow \mathbb  {R}$ from $\mathcal  {F}_{M^*,d^*,d,\alpha }^{\text  {(neural networks)}}$ with the structure $f(x) = \sum _{i=1}^3 \mu _i \cdot \sigma (\sum _{j=1}^4 \lambda _{i,j}\cdot \sigma (\sum _{v=1}^5 \theta _{i,j,v} \cdot x^{(v)}))$ (all weights with an index including zero neglected for a clear illustration). [Lifted from the paper]}}{15}{figure.2.1}\protected@file@percent }
\newlabel{fig:DLNPR-1}{{1}{15}{A not completely connected neural network $f:\SR ^5\rightarrow \SR $ from $\calF _{M^*,d^*,d,\alpha }^{\text {(neural networks)}}$ with the structure $f(x) = \sum _{i=1}^3 \mu _i \cdot \sigma (\sum _{j=1}^4 \lambda _{i,j}\cdot \sigma (\sum _{v=1}^5 \theta _{i,j,v} \cdot x^{(v)}))$ (all weights with an index including zero neglected for a clear illustration). [Lifted from the paper]}{figure.2.1}{}}
\newlabel{eq:DLNPR-6}{{6}{15}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Illustration of the components of a function from $\mathcal  {H}^{(l)}$ [Lifted from the paper]}}{16}{figure.2.2}\protected@file@percent }
\newlabel{fig:DLNPR-Fig2}{{2}{16}{Illustration of the components of a function from $\calH ^{(l)}$ [Lifted from the paper]}{figure.2.2}{}}
\newlabel{eq:DLNPR-7}{{7}{16}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.7}{}}
\newlabel{eq:DLNPR-8}{{8}{16}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.8}{}}
\newlabel{eq:DLNPR-9}{{9}{16}{Nonparametric Regression Estimation by Multilayer Feedforward Neural Networks}{equation.2.9}{}}
\newlabel{def:DLNPR-3}{{3}{17}{}{definition.2.3}{}}
\newlabel{thm:DLNPR-1}{{1}{17}{Main Result}{theorem.2.1}{}}
\newlabel{eq:DLNPR-10}{{10}{17}{Main Result}{equation.2.10}{}}
\newlabel{eq:DLNPR-11}{{11}{17}{Main Result}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Application to Simulated Data}{18}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Proofs}{18}{subsection.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}NPR Using Deep Neural Networks \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Johannes Schmidt-Hieber (ArXiv, 2017)}}{19}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Introduction}{19}{subsection.3.1}\protected@file@percent }
\newlabel{eq:sh-1}{{1}{19}{Introduction}{equation.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Mathematical Definition of Multilayer Neural Networks}{20}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Neural Network}{20}{paragraph*.3}\protected@file@percent }
\newlabel{eq:sh-2}{{2}{20}{Neural Network}{equation.3.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematical Modeling of Deep Network Characteristics}{20}{paragraph*.4}\protected@file@percent }
\newlabel{eq:sh-3}{{3}{20}{Mathematical Modeling of Deep Network Characteristics}{equation.3.3}{}}
\newlabel{eq:sh-4}{{4}{21}{Mathematical Modeling of Deep Network Characteristics}{equation.3.4}{}}
\newlabel{eq:sh-5}{{5}{21}{Mathematical Modeling of Deep Network Characteristics}{equation.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Main Results}{22}{subsection.3.3}\protected@file@percent }
\newlabel{eq:sh-6}{{6}{22}{Main Results}{equation.3.6}{}}
\newlabel{eq:sh-7}{{7}{23}{Main Results}{equation.3.7}{}}
\newlabel{thm:sh-1}{{1}{23}{Main Result}{theorem.3.1}{}}
\newlabel{eq:sh-8}{{8}{23}{Main Result}{equation.3.8}{}}
\newlabel{eq:sh-9}{{9}{23}{Main Result}{equation.3.9}{}}
\newlabel{cor:sh-1}{{1}{23}{}{corollary.3.1}{}}
\newlabel{eq:sh-10}{{10}{23}{}{equation.3.10}{}}
\newlabel{thm:sh-2}{{2}{24}{}{theorem.3.2}{}}
\newlabel{eq:sh-11}{{11}{24}{Main Results}{equation.3.11}{}}
\newlabel{thm:sh-3}{{3}{24}{}{theorem.3.3}{}}
\newlabel{lemma:sh-1}{{1}{25}{}{lemma.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Central Limit Theorems and Bootstrap in High Dimensions \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Victor Chernozhukov, Denis Chetverikov, Kengo Kato (AoP 2017)}}{26}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Introduction}{26}{subsection.4.1}\protected@file@percent }
\newlabel{eq:cck17-1}{{1}{26}{Introduction}{equation.4.1}{}}
\newlabel{eq:cck17-2}{{2}{26}{Introduction}{equation.4.2}{}}
\newlabel{eq:cck17-3}{{3}{26}{Introduction}{equation.4.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Some Notation}{27}{paragraph*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}High-dimensional CLT for hyperrectangles}{27}{subsection.4.2}\protected@file@percent }
\newlabel{eq:cck17-4}{{4}{27}{High-dimensional CLT for hyperrectangles}{equation.4.4}{}}
\newlabel{eq:cck17-5}{{5}{27}{High-dimensional CLT for hyperrectangles}{equation.4.5}{}}
\newlabel{thm:ckk17-1}{{1}{28}{Abstract high-dimensional CLT for hyper-rectangles}{theorem.4.1}{}}
\newlabel{eq:cck17-6}{{6}{28}{Abstract high-dimensional CLT for hyper-rectangles}{equation.4.6}{}}
\newlabel{eq:cck17-7}{{7}{28}{Abstract high-dimensional CLT for hyper-rectangles}{equation.4.7}{}}
\newlabel{eq:cck17-8}{{8}{28}{Key Features of Theorem \ref {thm:ckk17-1}}{equation.4.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}High-dimensional CLT for simple and sparsely convex sets}{28}{subsection.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.1}Simple Convex Sets}{28}{subsubsection.4.3.1}\protected@file@percent }
\newlabel{eq:cck17-9}{{9}{29}{Simple Convex Sets}{equation.4.9}{}}
\newlabel{eq:cck17-10}{{10}{29}{High-dimensional CLT for simple convex sets}{equation.4.10}{}}
\newlabel{eq:cck17-11}{{11}{29}{High-dimensional CLT for simple convex sets}{equation.4.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.3.2}Sparsely Convex Sets}{30}{subsubsection.4.3.2}\protected@file@percent }
\newlabel{def:cck17-3.1}{{1}{30}{Sparsely Convex Sets}{definition.4.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Sparse Principal Component Analysis \textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Hui Zou, Trevor Hastie, Robert Tisbirani (JCGS, 2006)}}{31}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Introduction}{31}{subsection.5.1}\protected@file@percent }
\newlabel{eq:scpa-1.1}{{1}{31}{Introduction}{equation.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Motivation and Details of SPCA}{31}{subsection.5.2}\protected@file@percent }
\newlabel{eq:spca-3.1}{{2}{32}{Motivation and Details of SPCA}{equation.5.2}{}}
\newlabel{eq:spca-3.2}{{3}{32}{Motivation and Details of SPCA}{equation.5.3}{}}
\newlabel{eq:spca-3.3}{{4}{32}{Motivation and Details of SPCA}{equation.5.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Direct Sparse Approximation}{32}{subsubsection.5.2.1}\protected@file@percent }
\newlabel{thm:spca-1}{{1}{32}{}{theorem.5.1}{}}
\newlabel{eq:spca-3.4}{{5}{32}{}{equation.5.5}{}}
\newlabel{eq:spca-3.5}{{6}{32}{Direct Sparse Approximation}{equation.5.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Sparse Principal Components Based on the SPCA Criterion}{32}{subsubsection.5.2.2}\protected@file@percent }
\newlabel{thm:spca-2}{{2}{33}{}{theorem.5.2}{}}
\newlabel{eq:spca-3.6}{{{7}}{33}{}{AMS.7}{}}
\newlabel{thm:spca-3}{{3}{33}{}{theorem.5.3}{}}
\newlabel{eq:spca-3.7}{{{8}}{33}{}{AMS.9}{}}
\newlabel{eq:spca-3.8}{{9}{33}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.9}{}}
\newlabel{eq:spca-3.9}{{10}{33}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.10}{}}
\newlabel{eq:spca-3.10}{{11}{33}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.11}{}}
\newlabel{eq:spca-3.11}{{12}{33}{Sparse Principal Components Based on the SPCA Criterion}{equation.5.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Deep IV\textit  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Jason Hartford, Greg Lewis, Kevin Leyton Brown , Matt Taddy}}{34}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Introduction}{34}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Counterfactual Prediction}{34}{subsection.6.2}\protected@file@percent }
\newlabel{eq:deepIV-1}{{1}{34}{Counterfactual Prediction}{equation.6.1}{}}
\newlabel{eq:deepIV-2}{{2}{34}{Counterfactual Prediction}{equation.6.2}{}}
\newlabel{eq:deepIV-3}{{3}{34}{Counterfactual Prediction}{equation.6.3}{}}
\newlabel{eq:deepIV-5}{{4}{34}{Counterfactual Prediction}{equation.6.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Estimating and Validating DeepIV}{35}{subsection.6.3}\protected@file@percent }
\newlabel{eq:deepIV-6}{{5}{35}{Estimating and Validating DeepIV}{equation.6.5}{}}
\@writefile{toc}{\contentsline {paragraph}{First Stage: Treatment Network}{35}{paragraph*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Second Stage: Outcome Network}{35}{paragraph*.11}\protected@file@percent }
\newlabel{eq:deepIV-7}{{6}{35}{Second Stage: Outcome Network}{equation.6.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Optimization for DeepIV Networks}{35}{subsubsection.6.3.1}\protected@file@percent }
\newlabel{eq:deepIV-8}{{7}{36}{Optimization for DeepIV Networks}{equation.6.7}{}}
\newlabel{eq:deepIV-9}{{8}{36}{Optimization for DeepIV Networks}{equation.6.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Causal Forests \text  {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Stegan Wager and Susan Athey (JASA, 2018)}}{37}{section.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Introduction}{37}{subsection.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Causal Forests}{37}{subsection.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Treatment Estimation with Unconfoundedness}{37}{subsubsection.7.2.1}\protected@file@percent }
\newlabel{eq:cf-1}{{1}{37}{Treatment Estimation with Unconfoundedness}{equation.7.1}{}}
\newlabel{eq:cf-2}{{2}{37}{Treatment Estimation with Unconfoundedness}{Item.34}{}}
\newlabel{eq:cf-3}{{3}{38}{Treatment Estimation with Unconfoundedness}{equation.7.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}From Regression Trees to Causal Forests}{38}{subsubsection.7.2.2}\protected@file@percent }
\newlabel{eq:cf-4}{{4}{38}{From Regression Trees to Causal Forests}{equation.7.4}{}}
\newlabel{eq:cf-5}{{5}{38}{From Regression Trees to Causal Forests}{equation.7.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Asymptotic Inference with Causal Forests}{38}{subsubsection.7.2.3}\protected@file@percent }
\newlabel{eq:cf-6}{{6}{38}{Asymptotic Inference with Causal Forests}{equation.7.6}{}}
\newlabel{eq:cf-7}{{7}{39}{Asymptotic Inference with Causal Forests}{equation.7.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.4}Honest Trees and Forests}{39}{subsubsection.7.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8}C-alpha Tests and Their Use {\relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8.5\p@ plus3\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip Jerzy Neyman, (IJS, 1979)}}{40}{section.8}\protected@file@percent }
